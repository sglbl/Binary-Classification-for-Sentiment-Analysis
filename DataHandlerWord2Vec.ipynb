{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f2665e87",
      "metadata": {
        "id": "f2665e87"
      },
      "source": [
        "# DATA CLEANSING\n",
        "---\n",
        "##   PART1: DATASET HANDLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "bbd098b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbd098b6",
        "outputId": "e0af1c00-0687-4a1c-d8df-0f840577afcc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'reviewerName', 'overall', 'reviewText', 'reviewTime',\n",
              "       'day_diff', 'helpful_yes', 'helpful_no', 'total_vote',\n",
              "       'score_pos_neg_diff', 'score_average_rating', 'wilson_lower_bound'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# jupyter run .\\DataHandlerWord2Vec.ipynb\n",
        "\n",
        "''' Importing dataset using pandas '''\n",
        "\n",
        "dataset = pd.read_csv('amazon_reviews.csv')\n",
        "# Getting \"reviewText\" column\n",
        "reviewText = dataset['reviewText']\n",
        "\n",
        "dataset.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "38872177",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38872177",
        "outputId": "3b8facae-2d1e-432f-e868-5d8822155484"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "''' Set punctuations and stop words '''\n",
        "\n",
        "# Downloading stopwords from nltk library\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Remove the NaN values from the dataset\n",
        "reviewText = reviewText.dropna()\n",
        "\n",
        "# Setting stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Except \"no\" from the stopwords\n",
        "stop_words.remove('no')\n",
        "\n",
        "# Set punctuations\n",
        "punctuations = set(punctuation)\n",
        "# Exceptions on punctuations\n",
        "punctuations.remove('.' and ',' and '!' and '?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4-lPV-xKlIYm",
      "metadata": {
        "id": "4-lPV-xKlIYm"
      },
      "outputs": [],
      "source": [
        "''' Remove punctuations and stop words from the reviews '''\n",
        "\n",
        "# Applying removed punctuations\n",
        "reviewText = reviewText.apply(lambda x: ''.join([word for word in x \n",
        "                                                  if word not in punctuations\n",
        "                                                  and word not in stop_words]))\n",
        "dataset['reviewText'] = reviewText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e578c3c7",
      "metadata": {
        "id": "e578c3c7"
      },
      "outputs": [],
      "source": [
        "# run the pip command on terminal or here\n",
        "# %pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz --user\n",
        "import spacy\n",
        "\n",
        "lem = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "def spacy_text_lemmatizer(text):\n",
        "    ''' Spacy Lemmatizer '''\n",
        "    text = str(text).lower()\n",
        "    text = lem(text)\n",
        "    # convert spacy.tokens.doc.Doc to str\n",
        "    text = \" \".join([token.lemma_ for token in text])\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "39501ebe",
      "metadata": {
        "id": "39501ebe"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "''' Load pretrained Word2Vec from tensorflow_hub '''\n",
        "\n",
        "word2vecPretrained = hub.load(\"https://tfhub.dev/google/Wiki-words-250/2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "00129edb",
      "metadata": {
        "id": "00129edb"
      },
      "outputs": [],
      "source": [
        "def get_word2vec_enc(reviews):\n",
        "    \"\"\" get word2vec value for each word in sentence and join so we use as RNN input \"\"\"\n",
        "    encoded_reviews = []\n",
        "    for review in reviews:\n",
        "        if type(review) == float or review == \"\": # if review is empty, make it \" \"\n",
        "            review = \" \" \n",
        "        tokens = review.split(\" \")\n",
        "        word2vec_embedding = word2vecPretrained(tokens) # use word2vec model\n",
        "        encoded_reviews.append(word2vec_embedding)\n",
        "    return encoded_reviews\n",
        "        \n",
        "def get_padded_encoded_reviews(encoded_reviews):\n",
        "    \"\"\" make all the encoded sentences same length (50)\"\"\"\n",
        "    max_length = 50 # max number of words in a sentence\n",
        "    padded_reviews_encoding = []\n",
        "    for enc_review in encoded_reviews:\n",
        "        if len(enc_review) > max_length: # if length is bigger than 50 just truncate\n",
        "            enc_review = enc_review[:max_length]\n",
        "        zero_padding_cnt = max_length - enc_review.shape[0]\n",
        "        pad = np.zeros((1, 250))\n",
        "        for i in range(zero_padding_cnt):\n",
        "            enc_review = np.concatenate((pad, enc_review), axis=0)\n",
        "        padded_reviews_encoding.append(enc_review)\n",
        "    return padded_reviews_encoding\n",
        "\n",
        "def rating_encode(rating):\n",
        "    \"\"\" return one hot encoding for rating value \"\"\"\n",
        "    if rating == '5.0':\n",
        "        return [1,0]\n",
        "    else: # rating == '1.0'\n",
        "        return [0,1]\n",
        "    \n",
        "def getTrainAndTestData(dataset):\n",
        "    ''' Split the data to train and test'''\n",
        "    # 90% of the dataset\n",
        "    train = dataset.sample(frac=0.9, random_state=100)\n",
        "    # 10% of the dataset\n",
        "    test = dataset.drop(train.index)\n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "id": "MwWpW1uQojtI",
      "metadata": {
        "id": "MwWpW1uQojtI"
      },
      "outputs": [],
      "source": [
        "def preprocess(df):\n",
        "    \"\"\" encode text value to numeric value (except 2.0 3.0 and 4.0 rating) \"\"\"\n",
        "    df = df[df['overall'] != 2.0]\n",
        "    df = df[df['overall'] != 3.0]\n",
        "    df = df[df['overall'] != 4.0]\n",
        "    # shape of dataframe is (3750, 12)\n",
        "\n",
        "    # apply spacy_text_lemmatizer to every review\n",
        "    df['reviewText'] = df['reviewText'].apply(spacy_text_lemmatizer)\n",
        "    reviews = df['reviewText'].tolist()\n",
        "    \n",
        "    # apply word2vec encoder to every review\n",
        "    encoded_reviews = get_word2vec_enc(reviews)\n",
        "    # apply padding to every encoded review to make them same length\n",
        "    padded_encoded_reviews = get_padded_encoded_reviews(encoded_reviews)\n",
        "    \n",
        "    # encoded rating\n",
        "    rates = df['overall'].tolist()\n",
        "    # make rates list of string to use it in rating_encode()\n",
        "    rates = [str(rate) for rate in rates]\n",
        "    \n",
        "    encoded_rating = [rating_encode(rate) for rate in rates]\n",
        "    X = np.array(padded_encoded_reviews)\n",
        "    Y = np.array(encoded_rating)\n",
        "    \n",
        "    return X, Y\n",
        "\n",
        "trainData, testData = getTrainAndTestData(dataset)\n",
        "train_X, train_Y = preprocess(trainData)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5a4e1ca",
      "metadata": {
        "id": "f5a4e1ca"
      },
      "source": [
        "---\n",
        "## PART2: DEEP LEARNING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "id": "f04d7ad8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f04d7ad8",
        "outputId": "e1dda6b0-5ee3-4df4-c932-903ca510bb70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting Train Data\n",
            "Epoch 1/5\n",
            "118/118 [==============================] - 7s 42ms/step - loss: 0.2857 - accuracy: 0.9392\n",
            "Epoch 2/5\n",
            "118/118 [==============================] - 5s 41ms/step - loss: 0.2269 - accuracy: 0.9400\n",
            "Epoch 3/5\n",
            "118/118 [==============================] - 5s 44ms/step - loss: 0.2247 - accuracy: 0.9400\n",
            "Epoch 4/5\n",
            "118/118 [==============================] - 6s 49ms/step - loss: 0.2245 - accuracy: 0.9400\n",
            "Epoch 5/5\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 0.2227 - accuracy: 0.9400\n",
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_36 (LSTM)               (None, 32)                36224     \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 36,290\n",
            "Trainable params: 36,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Evaluating Test Data\n",
            "13/13 - 1s - loss: 0.1872 - accuracy: 0.9543\n",
            "Test score: 0.18716658651828766 \n",
            "Test Accuracy: 0.9543269276618958\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "from tensorflow.python.keras.layers import Dense, LSTM\n",
        "from tensorflow.python.keras.engine.sequential import Sequential\n",
        "\n",
        "''' Build model using RNN + LSTM '''\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "# Train\n",
        "print('Fitting Train Data')\n",
        "model.fit(train_X, train_Y, epochs=5)\n",
        "model.summary()\n",
        "\n",
        "# Test\n",
        "print('Evaluating Test Data')\n",
        "test_X, test_Y = preprocess(testData)\n",
        "score, acc = model.evaluate(test_X, test_Y, verbose=2)\n",
        "print('Test score:' , score, \"\\nTest Accuracy:\", acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "e8c550be30345e4343f607af2e1377e672ed8219da2ec16773df1dcf745292ca"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
